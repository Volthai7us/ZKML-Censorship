{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:19:31.194159Z",
     "start_time": "2023-10-18T13:19:31.187753Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from skorch import NeuralNetClassifier, NeuralNetBinaryClassifier\n",
    "\n",
    "import json\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [
    {
     "data": {
      "text/plain": "    target                                               text\n0        1                           tommcfli like amsterdam?\n1        1                                     excit tonight!\n2        1  woo! finish game diana go tablet sandwich. awe...\n3        0                                         pleas talk\n4        1  shefali morn shefaly..th sun' alway blinding.....\n..     ...                                                ...\n95       1  carlosdejesu figur vid twitter yet. workin' th...\n96       0  sjc0815 well, 2605 briana shay offici gone fir...\n97       0  naturegrrl hard believe. &amp; hard believ see...\n98       0                      bust-up offic good start week\n99       0  someth tore tomato plant last night. immedi su...\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>tommcfli like amsterdam?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>excit tonight!</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>woo! finish game diana go tablet sandwich. awe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>pleas talk</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>shefali morn shefaly..th sun' alway blinding.....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>1</td>\n      <td>carlosdejesu figur vid twitter yet. workin' th...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>0</td>\n      <td>sjc0815 well, 2605 briana shay offici gone fir...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>0</td>\n      <td>naturegrrl hard believe. &amp;amp; hard believ see...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>0</td>\n      <td>bust-up offic good start week</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>0</td>\n      <td>someth tore tomato plant last night. immedi su...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"text\"]\n",
    "\n",
    "data = pd.read_csv(\"./data/processed_full.csv\", sep=',')\n",
    "data = data[:30000]\n",
    "data.columns = DATASET_COLUMNS\n",
    "data = data[data['text'].notnull()]\n",
    "data.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:30:15.941240Z",
     "start_time": "2023-10-18T13:30:15.891938Z"
    }
   },
   "id": "b2f7ad9175c1d8fd"
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "def tokenize_texts(texts_list):\n",
    "    all_words = set()\n",
    "    for text in texts_list:\n",
    "        words = str(text).split()\n",
    "        all_words.update(words)\n",
    "\n",
    "    word_to_index = {word: i for i, word in enumerate(all_words)}\n",
    "\n",
    "    with open('word_to_index.pkl', 'wb') as f:  # Sözlüğü kaydet\n",
    "        pickle.dump(word_to_index, f)\n",
    "\n",
    "    tokenized_texts = []\n",
    "    for text in texts_list:\n",
    "        words = text.split()\n",
    "        tokenized_texts.append([word_to_index[word] for word in words])\n",
    "\n",
    "    return tokenized_texts, word_to_index\n",
    "\n",
    "def pad_tokenized_texts(tokenized_texts, max_length=None):\n",
    "    if not max_length:\n",
    "        max_length = max([len(text) for text in tokenized_texts])\n",
    "\n",
    "    padded_texts = []\n",
    "    for text in tokenized_texts:\n",
    "        if len(text) < max_length:\n",
    "            text += [0] * (max_length - len(text))\n",
    "        padded_texts.append(text)\n",
    "\n",
    "    return padded_texts\n",
    "\n",
    "def pad_tokenized_text(tokenized_text, max_length=None):\n",
    "    if len(tokenized_text) < max_length:\n",
    "        tokenized_text += [0] * (max_length - len(tokenized_text))\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "def get_tokenized_sentence(sentence, word_to_index):\n",
    "    words = sentence.split()\n",
    "    tokenized_sentence = [word_to_index.get(word, 0) for word in words]\n",
    "    return tokenized_sentence\n",
    "\n",
    "texts = data['text']\n",
    "labels = data['target']\n",
    "\n",
    "tokenized_texts, word_to_index = tokenize_texts(texts)\n",
    "padded_texts = pad_tokenized_texts(tokenized_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:30:17.205969Z",
     "start_time": "2023-10-18T13:30:17.076386Z"
    }
   },
   "id": "dd9f6f0908770eae"
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index) + 1\n",
    "max_length = len(padded_texts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:30:18.029228Z",
     "start_time": "2023-10-18T13:30:18.025002Z"
    }
   },
   "id": "e416fd3b7eac827a"
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [],
   "source": [
    "labels.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tensor_padded_texts = torch.tensor(padded_texts).long()\n",
    "\n",
    "labels_tensor = torch.tensor(labels.to_numpy()).float()  # or `.long()` if your labels are integers\n",
    "reshaped_labels = labels_tensor.view(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:35:53.093493Z",
     "start_time": "2023-10-18T13:35:53.090735Z"
    }
   },
   "id": "bcfa38dc005a190c"
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [],
   "source": [
    "class AdvancedModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(AdvancedModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.layer1 = nn.Linear(embedding_dim, 32)\n",
    "        self.layer4 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer4(x))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:40:00.561596Z",
     "start_time": "2023-10-18T13:40:00.558190Z"
    }
   },
   "id": "64464fd5e14da839"
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 0.6931425929069519\n",
      "Epoch 2/1000, Loss: 0.6966422200202942\n",
      "Epoch 3/1000, Loss: 0.6920924782752991\n",
      "Epoch 4/1000, Loss: 0.6917058825492859\n",
      "Epoch 5/1000, Loss: 0.6920918822288513\n",
      "Epoch 6/1000, Loss: 0.6908047199249268\n",
      "Epoch 7/1000, Loss: 0.6890093684196472\n",
      "Epoch 8/1000, Loss: 0.6873288154602051\n",
      "Epoch 9/1000, Loss: 0.6859438419342041\n",
      "Epoch 10/1000, Loss: 0.6845901012420654\n",
      "Epoch 11/1000, Loss: 0.6829667091369629\n",
      "Epoch 12/1000, Loss: 0.6808612942695618\n",
      "Epoch 13/1000, Loss: 0.6782693862915039\n",
      "Epoch 14/1000, Loss: 0.6753161549568176\n",
      "Epoch 15/1000, Loss: 0.6721377372741699\n",
      "Epoch 16/1000, Loss: 0.6688145995140076\n",
      "Epoch 17/1000, Loss: 0.6653598546981812\n",
      "Epoch 18/1000, Loss: 0.6617099642753601\n",
      "Epoch 19/1000, Loss: 0.657901406288147\n",
      "Epoch 20/1000, Loss: 0.6540579199790955\n",
      "Epoch 21/1000, Loss: 0.650206983089447\n",
      "Epoch 22/1000, Loss: 0.6463309526443481\n",
      "Epoch 23/1000, Loss: 0.6424964666366577\n",
      "Epoch 24/1000, Loss: 0.638778567314148\n",
      "Epoch 25/1000, Loss: 0.6351789832115173\n",
      "Epoch 26/1000, Loss: 0.6316826343536377\n",
      "Epoch 27/1000, Loss: 0.628358781337738\n",
      "Epoch 28/1000, Loss: 0.6252158880233765\n",
      "Epoch 29/1000, Loss: 0.6222195625305176\n",
      "Epoch 30/1000, Loss: 0.6194679737091064\n",
      "Epoch 31/1000, Loss: 0.6167902946472168\n",
      "Epoch 32/1000, Loss: 0.6143783926963806\n",
      "Epoch 33/1000, Loss: 0.6120882630348206\n",
      "Epoch 34/1000, Loss: 0.6100482940673828\n",
      "Epoch 35/1000, Loss: 0.6081468462944031\n",
      "Epoch 36/1000, Loss: 0.6064202189445496\n",
      "Epoch 37/1000, Loss: 0.604891836643219\n",
      "Epoch 38/1000, Loss: 0.6034837365150452\n",
      "Epoch 39/1000, Loss: 0.6022647619247437\n",
      "Epoch 40/1000, Loss: 0.6011090278625488\n",
      "Epoch 41/1000, Loss: 0.6001048684120178\n",
      "Epoch 42/1000, Loss: 0.5992120504379272\n",
      "Epoch 43/1000, Loss: 0.5984273552894592\n",
      "Epoch 44/1000, Loss: 0.5977328419685364\n",
      "Epoch 45/1000, Loss: 0.5971145629882812\n",
      "Epoch 46/1000, Loss: 0.5965675115585327\n",
      "Epoch 47/1000, Loss: 0.59610915184021\n",
      "Epoch 48/1000, Loss: 0.5957698822021484\n",
      "Epoch 49/1000, Loss: 0.5953999161720276\n",
      "Epoch 50/1000, Loss: 0.595090925693512\n",
      "Epoch 51/1000, Loss: 0.5948949456214905\n",
      "Epoch 52/1000, Loss: 0.5946473479270935\n",
      "Epoch 53/1000, Loss: 0.5945890545845032\n",
      "Epoch 54/1000, Loss: 0.5949429273605347\n",
      "Epoch 55/1000, Loss: 0.594196617603302\n",
      "Epoch 56/1000, Loss: 0.5944647192955017\n",
      "Epoch 57/1000, Loss: 0.5941526889801025\n",
      "Epoch 58/1000, Loss: 0.5938965082168579\n",
      "Epoch 59/1000, Loss: 0.5940636992454529\n",
      "Epoch 60/1000, Loss: 0.5936400890350342\n",
      "Epoch 61/1000, Loss: 0.5936731696128845\n",
      "Epoch 62/1000, Loss: 0.5936281681060791\n",
      "Epoch 63/1000, Loss: 0.5937715768814087\n",
      "Epoch 64/1000, Loss: 0.5933371782302856\n",
      "Epoch 65/1000, Loss: 0.5936039090156555\n",
      "Epoch 66/1000, Loss: 0.5932505130767822\n",
      "Epoch 67/1000, Loss: 0.5934638977050781\n",
      "Epoch 68/1000, Loss: 0.5931731462478638\n",
      "Epoch 69/1000, Loss: 0.5933090448379517\n",
      "Epoch 70/1000, Loss: 0.5931494832038879\n",
      "Epoch 71/1000, Loss: 0.5931755900382996\n",
      "Epoch 72/1000, Loss: 0.5931345224380493\n",
      "Epoch 73/1000, Loss: 0.5930609703063965\n",
      "Epoch 74/1000, Loss: 0.5931135416030884\n",
      "Epoch 75/1000, Loss: 0.5929895639419556\n",
      "Epoch 76/1000, Loss: 0.5930680632591248\n",
      "Epoch 77/1000, Loss: 0.5929496884346008\n",
      "Epoch 78/1000, Loss: 0.5930148959159851\n",
      "Epoch 79/1000, Loss: 0.5929244160652161\n",
      "Epoch 80/1000, Loss: 0.5929487943649292\n",
      "Epoch 81/1000, Loss: 0.5929122567176819\n",
      "Epoch 82/1000, Loss: 0.5928918123245239\n",
      "Epoch 83/1000, Loss: 0.5928964614868164\n",
      "Epoch 84/1000, Loss: 0.5928473472595215\n",
      "Epoch 85/1000, Loss: 0.5928676128387451\n",
      "Epoch 86/1000, Loss: 0.5928160548210144\n",
      "Epoch 87/1000, Loss: 0.5928294658660889\n",
      "Epoch 88/1000, Loss: 0.5927958488464355\n",
      "Epoch 89/1000, Loss: 0.5927929878234863\n",
      "Epoch 90/1000, Loss: 0.592780351638794\n",
      "Epoch 91/1000, Loss: 0.5927621126174927\n",
      "Epoch 92/1000, Loss: 0.5927627682685852\n",
      "Epoch 93/1000, Loss: 0.5927367806434631\n",
      "Epoch 94/1000, Loss: 0.5927409529685974\n",
      "Epoch 95/1000, Loss: 0.5927165150642395\n",
      "Epoch 96/1000, Loss: 0.5927183032035828\n",
      "Epoch 97/1000, Loss: 0.5927009582519531\n",
      "Epoch 98/1000, Loss: 0.5926969647407532\n",
      "Epoch 99/1000, Loss: 0.5926860570907593\n",
      "Epoch 100/1000, Loss: 0.592674732208252\n",
      "Epoch 101/1000, Loss: 0.5926666259765625\n",
      "Epoch 102/1000, Loss: 0.5926506519317627\n",
      "Epoch 103/1000, Loss: 0.5926452279090881\n",
      "Epoch 104/1000, Loss: 0.5926306843757629\n",
      "Epoch 105/1000, Loss: 0.5926277041435242\n",
      "Epoch 106/1000, Loss: 0.5926156044006348\n",
      "Epoch 107/1000, Loss: 0.5926117300987244\n",
      "Epoch 108/1000, Loss: 0.5926008224487305\n",
      "Epoch 109/1000, Loss: 0.5925955176353455\n",
      "Epoch 110/1000, Loss: 0.5925897359848022\n",
      "Epoch 111/1000, Loss: 0.5925825238227844\n",
      "Epoch 112/1000, Loss: 0.5925751328468323\n",
      "Epoch 113/1000, Loss: 0.5925694108009338\n",
      "Epoch 114/1000, Loss: 0.5925661325454712\n",
      "Epoch 115/1000, Loss: 0.5925617814064026\n",
      "Epoch 116/1000, Loss: 0.5925585031509399\n",
      "Epoch 117/1000, Loss: 0.5925531983375549\n",
      "Epoch 118/1000, Loss: 0.5925489664077759\n",
      "Epoch 119/1000, Loss: 0.5925427079200745\n",
      "Epoch 120/1000, Loss: 0.5925381183624268\n",
      "Epoch 121/1000, Loss: 0.5925323367118835\n",
      "Epoch 122/1000, Loss: 0.5925287008285522\n",
      "Epoch 123/1000, Loss: 0.5925244688987732\n",
      "Epoch 124/1000, Loss: 0.5925217270851135\n",
      "Epoch 125/1000, Loss: 0.5925183892250061\n",
      "Epoch 126/1000, Loss: 0.592516303062439\n",
      "Epoch 127/1000, Loss: 0.5925136208534241\n",
      "Epoch 128/1000, Loss: 0.5925108790397644\n",
      "Epoch 129/1000, Loss: 0.5925061702728271\n",
      "Epoch 130/1000, Loss: 0.5925008058547974\n",
      "Epoch 131/1000, Loss: 0.5924942493438721\n",
      "Epoch 132/1000, Loss: 0.5924876928329468\n",
      "Epoch 133/1000, Loss: 0.5924811959266663\n",
      "Epoch 134/1000, Loss: 0.5924761891365051\n",
      "Epoch 135/1000, Loss: 0.5924725532531738\n",
      "Epoch 136/1000, Loss: 0.5924696326255798\n",
      "Epoch 137/1000, Loss: 0.5924661755561829\n",
      "Epoch 138/1000, Loss: 0.5924625992774963\n",
      "Epoch 139/1000, Loss: 0.5924586057662964\n",
      "Epoch 140/1000, Loss: 0.5924546718597412\n",
      "Epoch 141/1000, Loss: 0.5924507975578308\n",
      "Epoch 142/1000, Loss: 0.5924473404884338\n",
      "Epoch 143/1000, Loss: 0.5924439430236816\n",
      "Epoch 144/1000, Loss: 0.5924410223960876\n",
      "Epoch 145/1000, Loss: 0.5924385786056519\n",
      "Epoch 146/1000, Loss: 0.5924363732337952\n",
      "Epoch 147/1000, Loss: 0.5924339294433594\n",
      "Epoch 148/1000, Loss: 0.592430830001831\n",
      "Epoch 149/1000, Loss: 0.5924267768859863\n",
      "Epoch 150/1000, Loss: 0.59242182970047\n",
      "Epoch 151/1000, Loss: 0.592416524887085\n",
      "Epoch 152/1000, Loss: 0.5924116969108582\n",
      "Epoch 153/1000, Loss: 0.5924074649810791\n",
      "Epoch 154/1000, Loss: 0.5924031734466553\n",
      "Epoch 155/1000, Loss: 0.5923987030982971\n",
      "Epoch 156/1000, Loss: 0.5923939347267151\n",
      "Epoch 157/1000, Loss: 0.5923900604248047\n",
      "Epoch 158/1000, Loss: 0.592387318611145\n",
      "Epoch 159/1000, Loss: 0.592385470867157\n",
      "Epoch 160/1000, Loss: 0.5923842787742615\n",
      "Epoch 161/1000, Loss: 0.5923835039138794\n",
      "Epoch 162/1000, Loss: 0.5923827290534973\n",
      "Epoch 163/1000, Loss: 0.5923818945884705\n",
      "Epoch 164/1000, Loss: 0.5923810005187988\n",
      "Epoch 165/1000, Loss: 0.592380166053772\n",
      "Epoch 166/1000, Loss: 0.5923793315887451\n",
      "Epoch 167/1000, Loss: 0.5923783183097839\n",
      "Epoch 168/1000, Loss: 0.5923768877983093\n",
      "Epoch 169/1000, Loss: 0.592374861240387\n",
      "Epoch 170/1000, Loss: 0.5923722386360168\n",
      "Epoch 171/1000, Loss: 0.5923689603805542\n",
      "Epoch 172/1000, Loss: 0.5923652648925781\n",
      "Epoch 173/1000, Loss: 0.5923610925674438\n",
      "Epoch 174/1000, Loss: 0.5923561453819275\n",
      "Epoch 175/1000, Loss: 0.5923511385917664\n",
      "Epoch 176/1000, Loss: 0.5923465490341187\n",
      "Epoch 177/1000, Loss: 0.5923422574996948\n",
      "Epoch 178/1000, Loss: 0.5923382639884949\n",
      "Epoch 179/1000, Loss: 0.5923344492912292\n",
      "Epoch 180/1000, Loss: 0.5923311114311218\n",
      "Epoch 181/1000, Loss: 0.5923286080360413\n",
      "Epoch 182/1000, Loss: 0.5923267602920532\n",
      "Epoch 183/1000, Loss: 0.5923252701759338\n",
      "Epoch 184/1000, Loss: 0.5923240184783936\n",
      "Epoch 185/1000, Loss: 0.5923226475715637\n",
      "Epoch 186/1000, Loss: 0.5923207998275757\n",
      "Epoch 187/1000, Loss: 0.5923177599906921\n",
      "Epoch 188/1000, Loss: 0.5923126339912415\n",
      "Epoch 189/1000, Loss: 0.5923057794570923\n",
      "Epoch 190/1000, Loss: 0.5922992825508118\n",
      "Epoch 191/1000, Loss: 0.5922942161560059\n",
      "Epoch 192/1000, Loss: 0.5922901630401611\n",
      "Epoch 193/1000, Loss: 0.5922872424125671\n",
      "Epoch 194/1000, Loss: 0.5922848582267761\n",
      "Epoch 195/1000, Loss: 0.5922828912734985\n",
      "Epoch 196/1000, Loss: 0.5922811031341553\n",
      "Epoch 197/1000, Loss: 0.5922791957855225\n",
      "Epoch 198/1000, Loss: 0.5922772288322449\n",
      "Epoch 199/1000, Loss: 0.5922752618789673\n",
      "Epoch 200/1000, Loss: 0.5922732353210449\n",
      "Epoch 201/1000, Loss: 0.5922712683677673\n",
      "Epoch 202/1000, Loss: 0.5922693014144897\n",
      "Epoch 203/1000, Loss: 0.5922673344612122\n",
      "Epoch 204/1000, Loss: 0.592265248298645\n",
      "Epoch 205/1000, Loss: 0.5922633409500122\n",
      "Epoch 206/1000, Loss: 0.592261552810669\n",
      "Epoch 207/1000, Loss: 0.5922596454620361\n",
      "Epoch 208/1000, Loss: 0.5922572612762451\n",
      "Epoch 209/1000, Loss: 0.5922542214393616\n",
      "Epoch 210/1000, Loss: 0.5922504663467407\n",
      "Epoch 211/1000, Loss: 0.5922466516494751\n",
      "Epoch 212/1000, Loss: 0.5922432541847229\n",
      "Epoch 213/1000, Loss: 0.5922403335571289\n",
      "Epoch 214/1000, Loss: 0.5922372937202454\n",
      "Epoch 215/1000, Loss: 0.5922343134880066\n",
      "Epoch 216/1000, Loss: 0.5922321081161499\n",
      "Epoch 217/1000, Loss: 0.5922309160232544\n",
      "Epoch 218/1000, Loss: 0.5922302007675171\n",
      "Epoch 219/1000, Loss: 0.5922296047210693\n",
      "Epoch 220/1000, Loss: 0.5922288298606873\n",
      "Epoch 221/1000, Loss: 0.5922276377677917\n",
      "Epoch 222/1000, Loss: 0.5922266244888306\n",
      "Epoch 223/1000, Loss: 0.5922257304191589\n",
      "Epoch 224/1000, Loss: 0.5922244787216187\n",
      "Epoch 225/1000, Loss: 0.5922226905822754\n",
      "Epoch 226/1000, Loss: 0.5922208428382874\n",
      "Epoch 227/1000, Loss: 0.5922187566757202\n",
      "Epoch 228/1000, Loss: 0.5922164916992188\n",
      "Epoch 229/1000, Loss: 0.5922142863273621\n",
      "Epoch 230/1000, Loss: 0.5922145843505859\n",
      "Epoch 231/1000, Loss: 0.5922124981880188\n",
      "Epoch 232/1000, Loss: 0.5922099947929382\n",
      "Epoch 233/1000, Loss: 0.5922086834907532\n",
      "Epoch 234/1000, Loss: 0.5922079086303711\n",
      "Epoch 235/1000, Loss: 0.592207133769989\n",
      "Epoch 236/1000, Loss: 0.5922060608863831\n",
      "Epoch 237/1000, Loss: 0.5922045707702637\n",
      "Epoch 238/1000, Loss: 0.59220290184021\n",
      "Epoch 239/1000, Loss: 0.5922010540962219\n",
      "Epoch 240/1000, Loss: 0.5921990871429443\n",
      "Epoch 241/1000, Loss: 0.5921971201896667\n",
      "Epoch 242/1000, Loss: 0.5921947956085205\n",
      "Epoch 243/1000, Loss: 0.5921926498413086\n",
      "Epoch 244/1000, Loss: 0.5921909213066101\n",
      "Epoch 245/1000, Loss: 0.5921894311904907\n",
      "Epoch 246/1000, Loss: 0.5921881794929504\n",
      "Epoch 247/1000, Loss: 0.5921871662139893\n",
      "Epoch 248/1000, Loss: 0.5921863913536072\n",
      "Epoch 249/1000, Loss: 0.5921854972839355\n",
      "Epoch 250/1000, Loss: 0.5921842455863953\n",
      "Epoch 251/1000, Loss: 0.5921825170516968\n",
      "Epoch 252/1000, Loss: 0.5921806693077087\n",
      "Epoch 253/1000, Loss: 0.5921792984008789\n",
      "Epoch 254/1000, Loss: 0.5921781063079834\n",
      "Epoch 255/1000, Loss: 0.5921773314476013\n",
      "Epoch 256/1000, Loss: 0.5921769142150879\n",
      "Epoch 257/1000, Loss: 0.5921762585639954\n",
      "Epoch 258/1000, Loss: 0.5921754837036133\n",
      "Epoch 259/1000, Loss: 0.5921747088432312\n",
      "Epoch 260/1000, Loss: 0.5921741724014282\n",
      "Epoch 261/1000, Loss: 0.59217369556427\n",
      "Epoch 262/1000, Loss: 0.5921727418899536\n",
      "Epoch 263/1000, Loss: 0.5921710729598999\n",
      "Epoch 264/1000, Loss: 0.5921690464019775\n",
      "Epoch 265/1000, Loss: 0.5921674966812134\n",
      "Epoch 266/1000, Loss: 0.5921663045883179\n",
      "Epoch 267/1000, Loss: 0.5921648144721985\n",
      "Epoch 268/1000, Loss: 0.5921632051467896\n",
      "Epoch 269/1000, Loss: 0.5921616554260254\n",
      "Epoch 270/1000, Loss: 0.5921602249145508\n",
      "Epoch 271/1000, Loss: 0.592158854007721\n",
      "Epoch 272/1000, Loss: 0.5921576023101807\n",
      "Epoch 273/1000, Loss: 0.5921565294265747\n",
      "Epoch 274/1000, Loss: 0.5921556353569031\n",
      "Epoch 275/1000, Loss: 0.5921549797058105\n",
      "Epoch 276/1000, Loss: 0.5921544432640076\n",
      "Epoch 277/1000, Loss: 0.5921540856361389\n",
      "Epoch 278/1000, Loss: 0.5921533107757568\n",
      "Epoch 279/1000, Loss: 0.5921522378921509\n",
      "Epoch 280/1000, Loss: 0.5921512246131897\n",
      "Epoch 281/1000, Loss: 0.5921502709388733\n",
      "Epoch 282/1000, Loss: 0.5921493768692017\n",
      "Epoch 283/1000, Loss: 0.5921487212181091\n",
      "Epoch 284/1000, Loss: 0.5921480655670166\n",
      "Epoch 285/1000, Loss: 0.592147171497345\n",
      "Epoch 286/1000, Loss: 0.592146098613739\n",
      "Epoch 287/1000, Loss: 0.5921452045440674\n",
      "Epoch 288/1000, Loss: 0.5921445488929749\n",
      "Epoch 289/1000, Loss: 0.5921435356140137\n",
      "Epoch 290/1000, Loss: 0.5921422243118286\n",
      "Epoch 291/1000, Loss: 0.592140793800354\n",
      "Epoch 292/1000, Loss: 0.5921393036842346\n",
      "Epoch 293/1000, Loss: 0.5921372771263123\n",
      "Epoch 294/1000, Loss: 0.5921351909637451\n",
      "Epoch 295/1000, Loss: 0.5921334624290466\n",
      "Epoch 296/1000, Loss: 0.592132568359375\n",
      "Epoch 297/1000, Loss: 0.5921320915222168\n",
      "Epoch 298/1000, Loss: 0.5921317338943481\n",
      "Epoch 299/1000, Loss: 0.592130720615387\n",
      "Epoch 300/1000, Loss: 0.5921294093132019\n",
      "Epoch 301/1000, Loss: 0.5921278595924377\n",
      "Epoch 302/1000, Loss: 0.5921268463134766\n",
      "Epoch 303/1000, Loss: 0.5921258926391602\n",
      "Epoch 304/1000, Loss: 0.5921253561973572\n",
      "Epoch 305/1000, Loss: 0.5921249985694885\n",
      "Epoch 306/1000, Loss: 0.5921245217323303\n",
      "Epoch 307/1000, Loss: 0.592123806476593\n",
      "Epoch 308/1000, Loss: 0.5921227335929871\n",
      "Epoch 309/1000, Loss: 0.5921215415000916\n",
      "Epoch 310/1000, Loss: 0.5921204686164856\n",
      "Epoch 311/1000, Loss: 0.5921193957328796\n",
      "Epoch 312/1000, Loss: 0.5921181440353394\n",
      "Epoch 313/1000, Loss: 0.5921170711517334\n",
      "Epoch 314/1000, Loss: 0.5921164155006409\n",
      "Epoch 315/1000, Loss: 0.5921158790588379\n",
      "Epoch 316/1000, Loss: 0.5921151638031006\n",
      "Epoch 317/1000, Loss: 0.5921139121055603\n",
      "Epoch 318/1000, Loss: 0.5921124219894409\n",
      "Epoch 319/1000, Loss: 0.5921105742454529\n",
      "Epoch 320/1000, Loss: 0.5921087265014648\n",
      "Epoch 321/1000, Loss: 0.5921077132225037\n",
      "Epoch 322/1000, Loss: 0.5921070575714111\n",
      "Epoch 323/1000, Loss: 0.5921064019203186\n",
      "Epoch 324/1000, Loss: 0.5921053886413574\n",
      "Epoch 325/1000, Loss: 0.5921040177345276\n",
      "Epoch 326/1000, Loss: 0.5921029448509216\n",
      "Epoch 327/1000, Loss: 0.5921021699905396\n",
      "Epoch 328/1000, Loss: 0.5921013951301575\n",
      "Epoch 329/1000, Loss: 0.5921007394790649\n",
      "Epoch 330/1000, Loss: 0.5921002626419067\n",
      "Epoch 331/1000, Loss: 0.5921000242233276\n",
      "Epoch 332/1000, Loss: 0.5920998454093933\n",
      "Epoch 333/1000, Loss: 0.5920993089675903\n",
      "Epoch 334/1000, Loss: 0.5920985341072083\n",
      "Epoch 335/1000, Loss: 0.5920976400375366\n",
      "Epoch 336/1000, Loss: 0.5920966863632202\n",
      "Epoch 337/1000, Loss: 0.5920959115028381\n",
      "Epoch 338/1000, Loss: 0.5920954346656799\n",
      "Epoch 339/1000, Loss: 0.5920950770378113\n",
      "Epoch 340/1000, Loss: 0.5920948386192322\n",
      "Epoch 341/1000, Loss: 0.5920942425727844\n",
      "Epoch 342/1000, Loss: 0.5920934677124023\n",
      "Epoch 343/1000, Loss: 0.592092752456665\n",
      "Epoch 344/1000, Loss: 0.5920922756195068\n",
      "Epoch 345/1000, Loss: 0.5920916199684143\n",
      "Epoch 346/1000, Loss: 0.5920904278755188\n",
      "Epoch 347/1000, Loss: 0.5920887589454651\n",
      "Epoch 348/1000, Loss: 0.5920867919921875\n",
      "Epoch 349/1000, Loss: 0.5920853018760681\n",
      "Epoch 350/1000, Loss: 0.5920843482017517\n",
      "Epoch 351/1000, Loss: 0.5920838117599487\n",
      "Epoch 352/1000, Loss: 0.5920834541320801\n",
      "Epoch 353/1000, Loss: 0.5920831561088562\n",
      "Epoch 354/1000, Loss: 0.5920826196670532\n",
      "Epoch 355/1000, Loss: 0.5920818448066711\n",
      "Epoch 356/1000, Loss: 0.5920808911323547\n",
      "Epoch 357/1000, Loss: 0.5920798182487488\n",
      "Epoch 358/1000, Loss: 0.5920791029930115\n",
      "Epoch 359/1000, Loss: 0.5920785665512085\n",
      "Epoch 360/1000, Loss: 0.5920785069465637\n",
      "Epoch 361/1000, Loss: 0.592078447341919\n",
      "Epoch 362/1000, Loss: 0.5920780301094055\n",
      "Epoch 363/1000, Loss: 0.5920771956443787\n",
      "Epoch 364/1000, Loss: 0.5920765995979309\n",
      "Epoch 365/1000, Loss: 0.5920761823654175\n",
      "Epoch 366/1000, Loss: 0.5920761227607727\n",
      "Epoch 367/1000, Loss: 0.5920760631561279\n",
      "Epoch 368/1000, Loss: 0.5920759439468384\n",
      "Epoch 369/1000, Loss: 0.592075765132904\n",
      "Epoch 370/1000, Loss: 0.5920758843421936\n",
      "Epoch 371/1000, Loss: 0.5920757055282593\n",
      "Epoch 372/1000, Loss: 0.5920755863189697\n",
      "Epoch 373/1000, Loss: 0.592075526714325\n",
      "Epoch 374/1000, Loss: 0.592075526714325\n",
      "Epoch 375/1000, Loss: 0.592075526714325\n",
      "Epoch 376/1000, Loss: 0.5920754671096802\n",
      "Epoch 377/1000, Loss: 0.5920754671096802\n",
      "Epoch 378/1000, Loss: 0.5920754075050354\n",
      "Epoch 379/1000, Loss: 0.5920753479003906\n",
      "Epoch 380/1000, Loss: 0.5920751094818115\n",
      "Epoch 381/1000, Loss: 0.5920746922492981\n",
      "Epoch 382/1000, Loss: 0.5920735001564026\n",
      "Epoch 383/1000, Loss: 0.5920720100402832\n",
      "Epoch 384/1000, Loss: 0.5920707583427429\n",
      "Epoch 385/1000, Loss: 0.5920699238777161\n",
      "Epoch 386/1000, Loss: 0.5920692086219788\n",
      "Epoch 387/1000, Loss: 0.5920684337615967\n",
      "Epoch 388/1000, Loss: 0.5920670032501221\n",
      "Epoch 389/1000, Loss: 0.5920652151107788\n",
      "Epoch 390/1000, Loss: 0.5920637845993042\n",
      "Epoch 391/1000, Loss: 0.5920627117156982\n",
      "Epoch 392/1000, Loss: 0.5920612812042236\n",
      "Epoch 393/1000, Loss: 0.5920599102973938\n",
      "Epoch 394/1000, Loss: 0.5920588970184326\n",
      "Epoch 395/1000, Loss: 0.5920583009719849\n",
      "Epoch 396/1000, Loss: 0.5920581221580505\n",
      "Epoch 397/1000, Loss: 0.592058002948761\n",
      "Epoch 398/1000, Loss: 0.592058002948761\n",
      "Epoch 399/1000, Loss: 0.592058002948761\n",
      "Epoch 400/1000, Loss: 0.5920579433441162\n",
      "Epoch 401/1000, Loss: 0.5920578837394714\n",
      "Epoch 402/1000, Loss: 0.5920579433441162\n",
      "Epoch 403/1000, Loss: 0.5920578837394714\n",
      "Epoch 404/1000, Loss: 0.5920578241348267\n",
      "Epoch 405/1000, Loss: 0.5920577645301819\n",
      "Epoch 406/1000, Loss: 0.5920575857162476\n",
      "Epoch 407/1000, Loss: 0.5920572876930237\n",
      "Epoch 408/1000, Loss: 0.5920566916465759\n",
      "Epoch 409/1000, Loss: 0.5920557975769043\n",
      "Epoch 410/1000, Loss: 0.5920548439025879\n",
      "Epoch 411/1000, Loss: 0.5920538902282715\n",
      "Epoch 412/1000, Loss: 0.5920531153678894\n",
      "Epoch 413/1000, Loss: 0.5920524001121521\n",
      "Epoch 414/1000, Loss: 0.5920516848564148\n",
      "Epoch 415/1000, Loss: 0.5920512080192566\n",
      "Epoch 416/1000, Loss: 0.5920512080192566\n",
      "Epoch 417/1000, Loss: 0.592051088809967\n",
      "Epoch 418/1000, Loss: 0.5920510292053223\n",
      "Epoch 419/1000, Loss: 0.5920509099960327\n",
      "Epoch 420/1000, Loss: 0.5920506715774536\n",
      "Epoch 421/1000, Loss: 0.5920501947402954\n",
      "Epoch 422/1000, Loss: 0.5920491218566895\n",
      "Epoch 423/1000, Loss: 0.5920471549034119\n",
      "Epoch 424/1000, Loss: 0.5920445322990417\n",
      "Epoch 425/1000, Loss: 0.5920419692993164\n",
      "Epoch 426/1000, Loss: 0.5920405387878418\n",
      "Epoch 427/1000, Loss: 0.5920401215553284\n",
      "Epoch 428/1000, Loss: 0.592039942741394\n",
      "Epoch 429/1000, Loss: 0.592039942741394\n",
      "Epoch 430/1000, Loss: 0.5920398831367493\n",
      "Epoch 431/1000, Loss: 0.5920398831367493\n",
      "Epoch 432/1000, Loss: 0.5920398831367493\n",
      "Epoch 433/1000, Loss: 0.5920398235321045\n",
      "Epoch 434/1000, Loss: 0.5920397639274597\n",
      "Epoch 435/1000, Loss: 0.5920395255088806\n",
      "Epoch 436/1000, Loss: 0.5920392274856567\n",
      "Epoch 437/1000, Loss: 0.5920385718345642\n",
      "Epoch 438/1000, Loss: 0.5920379161834717\n",
      "Epoch 439/1000, Loss: 0.5920376181602478\n",
      "Epoch 440/1000, Loss: 0.5920374989509583\n",
      "Epoch 441/1000, Loss: 0.5920373797416687\n",
      "Epoch 442/1000, Loss: 0.5920372605323792\n",
      "Epoch 443/1000, Loss: 0.5920370221138\n",
      "Epoch 444/1000, Loss: 0.5920363068580627\n",
      "Epoch 445/1000, Loss: 0.5920355916023254\n",
      "Epoch 446/1000, Loss: 0.5920352339744568\n",
      "Epoch 447/1000, Loss: 0.592035174369812\n",
      "Epoch 448/1000, Loss: 0.5920351147651672\n",
      "Epoch 449/1000, Loss: 0.5920350551605225\n",
      "Epoch 450/1000, Loss: 0.5920349359512329\n",
      "Epoch 451/1000, Loss: 0.5920346975326538\n",
      "Epoch 452/1000, Loss: 0.5920343399047852\n",
      "Epoch 453/1000, Loss: 0.5920335054397583\n",
      "Epoch 454/1000, Loss: 0.5920321345329285\n",
      "Epoch 455/1000, Loss: 0.5920308828353882\n",
      "Epoch 456/1000, Loss: 0.5920303463935852\n",
      "Epoch 457/1000, Loss: 0.5920301675796509\n",
      "Epoch 458/1000, Loss: 0.5920302271842957\n",
      "Epoch 459/1000, Loss: 0.5920303463935852\n",
      "Epoch 460/1000, Loss: 0.5920307636260986\n",
      "Epoch 461/1000, Loss: 0.5920314192771912\n",
      "Epoch 462/1000, Loss: 0.592032790184021\n",
      "Epoch 463/1000, Loss: 0.592035710811615\n",
      "Epoch 464/1000, Loss: 0.5920416712760925\n",
      "Epoch 465/1000, Loss: 0.5920538306236267\n",
      "Epoch 466/1000, Loss: 0.5920781493186951\n",
      "Epoch 467/1000, Loss: 0.5921227931976318\n",
      "Epoch 468/1000, Loss: 0.5921933650970459\n",
      "Epoch 469/1000, Loss: 0.5922662019729614\n",
      "Epoch 470/1000, Loss: 0.5922732949256897\n",
      "Epoch 471/1000, Loss: 0.5921630263328552\n",
      "Epoch 472/1000, Loss: 0.5920436978340149\n",
      "Epoch 473/1000, Loss: 0.5920536518096924\n",
      "Epoch 474/1000, Loss: 0.5921376347541809\n",
      "Epoch 475/1000, Loss: 0.5921366810798645\n",
      "Epoch 476/1000, Loss: 0.5920526385307312\n",
      "Epoch 477/1000, Loss: 0.5920384526252747\n",
      "Epoch 478/1000, Loss: 0.5920954942703247\n",
      "Epoch 479/1000, Loss: 0.592089831829071\n",
      "Epoch 480/1000, Loss: 0.592035710811615\n",
      "Epoch 481/1000, Loss: 0.5920444130897522\n",
      "Epoch 482/1000, Loss: 0.5920765995979309\n",
      "Epoch 483/1000, Loss: 0.5920502543449402\n",
      "Epoch 484/1000, Loss: 0.5920255184173584\n",
      "Epoch 485/1000, Loss: 0.5920494198799133\n",
      "Epoch 486/1000, Loss: 0.5920525789260864\n",
      "Epoch 487/1000, Loss: 0.5920267701148987\n",
      "Epoch 488/1000, Loss: 0.5920323133468628\n",
      "Epoch 489/1000, Loss: 0.5920454859733582\n",
      "Epoch 490/1000, Loss: 0.5920295119285583\n",
      "Epoch 491/1000, Loss: 0.5920237302780151\n",
      "Epoch 492/1000, Loss: 0.5920366644859314\n",
      "Epoch 493/1000, Loss: 0.5920313596725464\n",
      "Epoch 494/1000, Loss: 0.5920217037200928\n",
      "Epoch 495/1000, Loss: 0.5920296311378479\n",
      "Epoch 496/1000, Loss: 0.5920310020446777\n",
      "Epoch 497/1000, Loss: 0.5920222997665405\n",
      "Epoch 498/1000, Loss: 0.5920252203941345\n",
      "Epoch 499/1000, Loss: 0.5920292735099792\n",
      "Epoch 500/1000, Loss: 0.5920233130455017\n",
      "Epoch 501/1000, Loss: 0.5920225977897644\n",
      "Epoch 502/1000, Loss: 0.5920268297195435\n",
      "Epoch 503/1000, Loss: 0.592023491859436\n",
      "Epoch 504/1000, Loss: 0.5920203924179077\n",
      "Epoch 505/1000, Loss: 0.5920230150222778\n",
      "Epoch 506/1000, Loss: 0.5920220613479614\n",
      "Epoch 507/1000, Loss: 0.5920192003250122\n",
      "Epoch 508/1000, Loss: 0.5920209288597107\n",
      "Epoch 509/1000, Loss: 0.592021644115448\n",
      "Epoch 510/1000, Loss: 0.5920192003250122\n",
      "Epoch 511/1000, Loss: 0.5920191407203674\n",
      "Epoch 512/1000, Loss: 0.5920196771621704\n",
      "Epoch 513/1000, Loss: 0.5920178294181824\n",
      "Epoch 514/1000, Loss: 0.5920170545578003\n",
      "Epoch 515/1000, Loss: 0.5920182466506958\n",
      "Epoch 516/1000, Loss: 0.5920177102088928\n",
      "Epoch 517/1000, Loss: 0.5920168161392212\n",
      "Epoch 518/1000, Loss: 0.5920174717903137\n",
      "Epoch 519/1000, Loss: 0.5920177102088928\n",
      "Epoch 520/1000, Loss: 0.5920168161392212\n",
      "Epoch 521/1000, Loss: 0.592016875743866\n",
      "Epoch 522/1000, Loss: 0.5920171737670898\n",
      "Epoch 523/1000, Loss: 0.5920162796974182\n",
      "Epoch 524/1000, Loss: 0.5920152068138123\n",
      "Epoch 525/1000, Loss: 0.592014729976654\n",
      "Epoch 526/1000, Loss: 0.5920137763023376\n",
      "Epoch 527/1000, Loss: 0.5920127630233765\n",
      "Epoch 528/1000, Loss: 0.5920127630233765\n",
      "Epoch 529/1000, Loss: 0.5920125246047974\n",
      "Epoch 530/1000, Loss: 0.5920113921165466\n",
      "Epoch 531/1000, Loss: 0.5920102000236511\n",
      "Epoch 532/1000, Loss: 0.5920093059539795\n",
      "Epoch 533/1000, Loss: 0.592008650302887\n",
      "Epoch 534/1000, Loss: 0.5920083522796631\n",
      "Epoch 535/1000, Loss: 0.592008113861084\n",
      "Epoch 536/1000, Loss: 0.5920079350471497\n",
      "Epoch 537/1000, Loss: 0.5920076966285706\n",
      "Epoch 538/1000, Loss: 0.5920076370239258\n",
      "Epoch 539/1000, Loss: 0.5920076966285706\n",
      "Epoch 540/1000, Loss: 0.5920075178146362\n",
      "Epoch 541/1000, Loss: 0.5920075178146362\n",
      "Epoch 542/1000, Loss: 0.5920074582099915\n",
      "Epoch 543/1000, Loss: 0.5920075178146362\n",
      "Epoch 544/1000, Loss: 0.5920075178146362\n",
      "Epoch 545/1000, Loss: 0.5920074582099915\n",
      "Epoch 546/1000, Loss: 0.5920075178146362\n",
      "Epoch 547/1000, Loss: 0.5920074582099915\n",
      "Epoch 548/1000, Loss: 0.5920073986053467\n",
      "Epoch 549/1000, Loss: 0.5920073986053467\n",
      "Epoch 550/1000, Loss: 0.5920074582099915\n",
      "Epoch 551/1000, Loss: 0.5920073986053467\n",
      "Epoch 552/1000, Loss: 0.5920073390007019\n",
      "Epoch 553/1000, Loss: 0.5920072793960571\n",
      "Epoch 554/1000, Loss: 0.5920072793960571\n",
      "Epoch 555/1000, Loss: 0.5920072197914124\n",
      "Epoch 556/1000, Loss: 0.5920071601867676\n",
      "Epoch 557/1000, Loss: 0.592007040977478\n",
      "Epoch 558/1000, Loss: 0.5920065641403198\n",
      "Epoch 559/1000, Loss: 0.5920059084892273\n",
      "Epoch 560/1000, Loss: 0.5920050144195557\n",
      "Epoch 561/1000, Loss: 0.5920041799545288\n",
      "Epoch 562/1000, Loss: 0.592003583908081\n",
      "Epoch 563/1000, Loss: 0.5920032262802124\n",
      "Epoch 564/1000, Loss: 0.5920029878616333\n",
      "Epoch 565/1000, Loss: 0.5920023918151855\n",
      "Epoch 566/1000, Loss: 0.5920015573501587\n",
      "Epoch 567/1000, Loss: 0.5920010805130005\n",
      "Epoch 568/1000, Loss: 0.5920007824897766\n",
      "Epoch 569/1000, Loss: 0.5920004844665527\n",
      "Epoch 570/1000, Loss: 0.5920000076293945\n",
      "Epoch 571/1000, Loss: 0.591999351978302\n",
      "Epoch 572/1000, Loss: 0.591998815536499\n",
      "Epoch 573/1000, Loss: 0.591998279094696\n",
      "Epoch 574/1000, Loss: 0.5919975638389587\n",
      "Epoch 575/1000, Loss: 0.5919963717460632\n",
      "Epoch 576/1000, Loss: 0.5919951796531677\n",
      "Epoch 577/1000, Loss: 0.5919942259788513\n",
      "Epoch 578/1000, Loss: 0.5919935703277588\n",
      "Epoch 579/1000, Loss: 0.5919927954673767\n",
      "Epoch 580/1000, Loss: 0.5919917821884155\n",
      "Epoch 581/1000, Loss: 0.5919904708862305\n",
      "Epoch 582/1000, Loss: 0.5919893980026245\n",
      "Epoch 583/1000, Loss: 0.5919890403747559\n",
      "Epoch 584/1000, Loss: 0.5919889211654663\n",
      "Epoch 585/1000, Loss: 0.5919889211654663\n",
      "Epoch 586/1000, Loss: 0.591988742351532\n",
      "Epoch 587/1000, Loss: 0.591988742351532\n",
      "Epoch 588/1000, Loss: 0.5919886827468872\n",
      "Epoch 589/1000, Loss: 0.5919886231422424\n",
      "Epoch 590/1000, Loss: 0.5919884443283081\n",
      "Epoch 591/1000, Loss: 0.5919879078865051\n",
      "Epoch 592/1000, Loss: 0.591987133026123\n",
      "Epoch 593/1000, Loss: 0.5919865965843201\n",
      "Epoch 594/1000, Loss: 0.5919864177703857\n",
      "Epoch 595/1000, Loss: 0.5919864177703857\n",
      "Epoch 596/1000, Loss: 0.5919862985610962\n",
      "Epoch 597/1000, Loss: 0.5919861793518066\n",
      "Epoch 598/1000, Loss: 0.5919858813285828\n",
      "Epoch 599/1000, Loss: 0.5919854044914246\n",
      "Epoch 600/1000, Loss: 0.5919842720031738\n",
      "Epoch 601/1000, Loss: 0.5919828414916992\n",
      "Epoch 602/1000, Loss: 0.5919816493988037\n",
      "Epoch 603/1000, Loss: 0.5919805765151978\n",
      "Epoch 604/1000, Loss: 0.5919800400733948\n",
      "Epoch 605/1000, Loss: 0.5919798612594604\n",
      "Epoch 606/1000, Loss: 0.5919798016548157\n",
      "Epoch 607/1000, Loss: 0.5919796824455261\n",
      "Epoch 608/1000, Loss: 0.5919798016548157\n",
      "Epoch 609/1000, Loss: 0.5919798016548157\n",
      "Epoch 610/1000, Loss: 0.5919796824455261\n",
      "Epoch 611/1000, Loss: 0.5919796228408813\n",
      "Epoch 612/1000, Loss: 0.5919796228408813\n",
      "Epoch 613/1000, Loss: 0.5919796824455261\n",
      "Epoch 614/1000, Loss: 0.5919796228408813\n",
      "Epoch 615/1000, Loss: 0.5919796228408813\n",
      "Epoch 616/1000, Loss: 0.5919795632362366\n",
      "Epoch 617/1000, Loss: 0.5919795632362366\n",
      "Epoch 618/1000, Loss: 0.5919795036315918\n",
      "Epoch 619/1000, Loss: 0.5919795036315918\n",
      "Epoch 620/1000, Loss: 0.5919795036315918\n",
      "Epoch 621/1000, Loss: 0.5919795036315918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[382], line 22\u001B[0m\n\u001B[1;32m     19\u001B[0m outputs \u001B[38;5;241m=\u001B[39m outputs[:, \u001B[38;5;241m0\u001B[39m, :]\n\u001B[1;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, reshaped_labels)\n\u001B[0;32m---> 22\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = max_length\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Create the model\n",
    "model = AdvancedModel(vocab_size, input_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(tensor_padded_texts)\n",
    "    outputs = torch.mean(outputs, dim=1, keepdim=True)\n",
    "    outputs = outputs[:, 0, :]\n",
    "\n",
    "    loss = criterion(outputs, reshaped_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:42:18.754079Z",
     "start_time": "2023-10-18T13:40:01.171940Z"
    }
   },
   "id": "7ce928b7c492de9a"
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Accuracy: 89.1369640827179%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tensor_padded_texts)\n",
    "    outputs = torch.mean(outputs, dim=1, keepdim=True)\n",
    "    outputs = outputs[:, 0, :]\n",
    "    predicted = (outputs >= 0.5).float()\n",
    "    accuracy = (predicted == reshaped_labels.float()).sum() / len(reshaped_labels)\n",
    "    \n",
    "print(f\"Training complete. Accuracy: {accuracy.item() * 100}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:43:35.586519Z",
     "start_time": "2023-10-18T13:43:35.399825Z"
    }
   },
   "id": "aa681cdbbf2ddc10"
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "outputs": [],
   "source": [
    "bad_sentence = \"dead dead dead fuck fuck die die \"\n",
    "good_sentence = \"i have killed to many stupids\"\n",
    "sentence = good_sentence\n",
    "with open('word_to_index.pkl', 'rb') as f:\n",
    "    loaded_word_to_index = pickle.load(f)\n",
    "\n",
    "tokenized_sentence = pad_tokenized_text(get_tokenized_sentence(sentence, loaded_word_to_index), max_length)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:46:12.274887Z",
     "start_time": "2023-10-18T13:46:12.251493Z"
    }
   },
   "id": "e76234f41f95204e"
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [],
   "source": [
    "single_sentence = np.array(tokenized_sentence).reshape(1, -1)\n",
    "scaled_single_sentence = scaler.transform(single_sentence)\n",
    "scaled_single_sentence = torch.tensor(scaled_single_sentence).float()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:46:12.681552Z",
     "start_time": "2023-10-18T13:46:12.678500Z"
    }
   },
   "id": "48750d5c44351621"
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fx/3kl00f6x4bq3c7ymjd8v16qr0000gn/T/ipykernel_49005/4270029453.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  single_sentence = torch.tensor(scaled_single_sentence).long().unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "single_sentence = torch.tensor(scaled_single_sentence).long().unsqueeze(0)\n",
    "model.eval()\n",
    "# Tahmin yap\n",
    "with torch.no_grad():\n",
    "    outputs = model(single_sentence)\n",
    "    outputs = torch.mean(outputs, dim=1, keepdim=True)\n",
    "    outputs = outputs[:, 0, :]\n",
    "    outputs = torch.mean(outputs, dim=1)\n",
    "    \n",
    "# Tahminin sonucunu ikili sınıflandırma için bir eşik değeriyle (threshold) karşılaştır\n",
    "predicted_label = (outputs >= 0.5).int().item()\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T13:46:13.165654Z",
     "start_time": "2023-10-18T13:46:13.161480Z"
    }
   },
   "id": "43a976a003d87d3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7a37c3f194b561b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
